{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b913dcff",
      "metadata": {
        "id": "b913dcff"
      },
      "source": [
        "# Cardiomegaly Detection with Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d32cef",
      "metadata": {
        "id": "a1d32cef"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb45413",
      "metadata": {
        "id": "7fb45413"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas numpy opencv-python matplotlib scikit-learn tensorflow Pillow seaborn albumentations \\\n",
        "                tqdm grad-cam jupyterlab pydicom imgaug scikit-image keras-tuner \\\n",
        "                mlflow optuna shap eli5 rich plotly\n",
        "\n",
        "\n",
        "# TensorFlow and Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#System Utilities\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Data Analysis and Plotting\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.pyplot import imread, imshow, subplots, show\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from rich import print as rprint\n",
        "\n",
        "# Image Processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage import exposure, filters, morphology, measure\n",
        "import imgaug.augmenters as iaa\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "\n",
        "# Utility & Performance Tracking\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Model Evaluation and Metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "\n",
        "# Data Augmentation (Advanced)\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Explainability\n",
        "#from gradcam import GradCAM, GradCAMPlusPlus\n",
        "#from gradcam.utils import visualize_cam\n",
        "\n",
        "# Hyper Parameter Tuning\n",
        "import keras_tuner as kt\n",
        "import optuna\n",
        "import mlflow\n",
        "mlflow.set_tracking_uri(\"file:///content/mlruns\")\n",
        "mlflow.set_experiment(\"Cardiomegaly_CELM\")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# GPU Configuration\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Allow dynamic memory growth\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"{len(gpus)} GPU(s) detected and configured.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Logging Configuration\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "logging.basicConfig(filename=os.path.join(log_dir, f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
        "                    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "print(\"Environment successfully set up and ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f073d83b",
      "metadata": {
        "id": "f073d83b"
      },
      "source": [
        "## 2. Load Combined Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5df07e",
      "metadata": {
        "id": "9e5df07e"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Load and combine multiple CSV files\n",
        "# -----------------------------\n",
        "csv_files = [\n",
        "    'combined_dataset.csv',\n",
        "    'padchest_labels.csv',\n",
        "    'nih_labels.csv',\n",
        "    'vindr_labels.csv',\n",
        "    'chexpert_labels.csv'\n",
        "]\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "for file in csv_files:\n",
        "    if os.path.exists(file):\n",
        "        df = pd.read_csv(file)\n",
        "        print(f\"{file} loaded with shape: {df.shape}\")\n",
        "        dataframes.append(df)\n",
        "    else:\n",
        "        print(f\" Warning: File {file} not found.\")\n",
        "\n",
        "# -----------------------------\n",
        "#  Concatenate all dataframes\n",
        "# -----------------------------\n",
        "full_df = pd.concat(dataframes, ignore_index=True)\n",
        "print(f\"\\n Full dataset shape after concatenation: {full_df.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Check for missing values\n",
        "# -----------------------------\n",
        "missing_info = full_df.isnull().sum()\n",
        "missing_info = missing_info[missing_info > 0]\n",
        "if not missing_info.empty:\n",
        "    print(\"\\n Missing values detected:\")\n",
        "    print(missing_info)\n",
        "else:\n",
        "    print(\"\\n No missing values found.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Analyze class distribution\n",
        "# -----------------------------\n",
        "print(\"\\n Class distribution:\")\n",
        "print(full_df['label'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=full_df, x='label', order=full_df['label'].value_counts().index, palette='viridis')\n",
        "plt.title('Class Distribution: Cardiomegaly vs Normal/Other Findings')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to disk\n",
        "plt.savefig('class_distribution.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a6e5354",
      "metadata": {
        "id": "1a6e5354"
      },
      "source": [
        "## 3. CLAHE and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3dc3b9",
      "metadata": {
        "id": "3c3dc3b9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# CLAHE function\n",
        "# -----------------------------\n",
        "def apply_clahe(image, clip_limit=7.0, tile_grid_size=(11, 11)):\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "    cl = clahe.apply(l)\n",
        "    limg = cv2.merge((cl, a, b))\n",
        "    return cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing for 1 image\n",
        "# -----------------------------\n",
        "def preprocess_image(image_path, output_size=(224, 224)):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\" Image not found: {image_path}\")\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = apply_clahe(img)\n",
        "    img = cv2.resize(img, output_size)\n",
        "    img = (img / 255.0 * 255).astype(np.uint8)  # normalize and convert back to uint8 for saving\n",
        "    return img\n",
        "\n",
        "# -----------------------------\n",
        "# Batch processing from CSV\n",
        "# -----------------------------\n",
        "def process_images_from_csv(csv_path, image_column, input_root=\"\", output_root=\"processed_images\", img_size=(224, 224)):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    image_paths = df[image_column].dropna().tolist()\n",
        "\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "    print(f\"\\n Processing {len(image_paths)} images...\")\n",
        "\n",
        "    for rel_path in tqdm(image_paths):\n",
        "        in_path = os.path.join(input_root, rel_path)\n",
        "        out_path = os.path.join(output_root, os.path.basename(rel_path))\n",
        "\n",
        "        try:\n",
        "            processed = preprocess_image(in_path, output_size=img_size)\n",
        "            cv2.imwrite(out_path, cv2.cvtColor(processed, cv2.COLOR_RGB2BGR))\n",
        "        except Exception as e:\n",
        "            print(f\" Skipping {rel_path}: {e}\")\n",
        "\n",
        "    print(f\"\\n Done. Processed images saved to: {output_root}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Run example\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file = \"combined_dataset.csv\"          # path to your CSV\n",
        "    image_col = \"image_path\"                   # column name that contains relative or full image paths\n",
        "    input_images_dir = \"raw_images\"            # optional prefix (if images are not in root)\n",
        "    output_images_dir = \"processed_images\"     # where to save processed versions\n",
        "    image_size = (224, 224)\n",
        "\n",
        "    process_images_from_csv(csv_file, image_col, input_images_dir, output_images_dir, image_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33cf73e5",
      "metadata": {
        "id": "33cf73e5"
      },
      "source": [
        "## 4. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7724bad6",
      "metadata": {
        "id": "7724bad6"
      },
      "outputs": [],
      "source": [
        "def get_keras_augmentation_pipeline(mode=\"medium\"):\n",
        "    \"\"\"\n",
        "    Returns a tf.keras.Sequential data augmentation pipeline.\n",
        "    Args:\n",
        "        mode (str): 'light', 'medium', or 'strong'\n",
        "    \"\"\"\n",
        "    if mode == \"light\":\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        ])\n",
        "    elif mode == \"medium\":\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.05),\n",
        "            tf.keras.layers.RandomZoom(height_factor=0.1, width_factor=0.1),\n",
        "            tf.keras.layers.RandomBrightness(factor=0.1),\n",
        "            tf.keras.layers.RandomContrast(factor=0.1),\n",
        "        ])\n",
        "    elif mode == \"strong\":\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            tf.keras.layers.RandomRotation(0.15),\n",
        "            tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "            tf.keras.layers.RandomBrightness(factor=0.2),\n",
        "            tf.keras.layers.RandomContrast(factor=0.3),\n",
        "            tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
        "        ])\n",
        "    else:\n",
        "        raise ValueError(\"Mode must be 'light', 'medium', or 'strong'.\")\n",
        "\n",
        "# Example usage\n",
        "data_augmentation = get_keras_augmentation_pipeline(\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_augmentations(image_path, n_examples=6):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(n_examples):\n",
        "        augmented_img = augment_image_np(image)\n",
        "        plt.subplot(1, n_examples, i + 1)\n",
        "        plt.imshow(augmented_img)\n",
        "        plt.title(f\"Aug #{i+1}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(\"Augmented Samples\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "151gXbR3Txec"
      },
      "id": "151gXbR3Txec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef get_albumentations_pipeline():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.Rotate(limit=15, p=0.5),\n",
        "        A.RandomScale(scale_limit=0.15, p=0.5),\n",
        "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n",
        "        A.GridDistortion(p=0.3),\n",
        "        A.RandomGamma(p=0.3),\n",
        "        A.GaussianBlur(blur_limit=(3,5), p=0.2),\n",
        "        A.Resize(224, 224),  # Resize after augmentation\n",
        "    ])\n",
        "\n",
        "def augment_image_np(image_np):\n",
        "    aug = get_albumentations_pipeline()\n",
        "    augmented = aug(image=image_np)\n",
        "    return augmented[\"image\"]\n"
      ],
      "metadata": {
        "id": "84KheKscTviN"
      },
      "id": "84KheKscTviN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "050e1c06",
      "metadata": {
        "id": "050e1c06"
      },
      "source": [
        "## 5. Create TensorFlow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# -----------------------------\n",
        "# CLAHE with OpenCV\n",
        "# -----------------------------\n",
        "import cv2\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def apply_clahe_tf(image):\n",
        "    \"\"\"Applies CLAHE on the L-channel of LAB image.\"\"\"\n",
        "    image = tf.numpy_function(apply_clahe_np, [image], tf.uint8)\n",
        "    return tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "def apply_clahe_np(image_np):\n",
        "    img_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(img_rgb)\n",
        "    clahe = cv2.createCLAHE(clipLimit=7.0, tileGridSize=(11,11))\n",
        "    cl = clahe.apply(l)\n",
        "    merged = cv2.merge((cl, a, b))\n",
        "    result = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "    return result\n",
        "\n",
        "# -----------------------------\n",
        "# Data Augmentation Layer\n",
        "# -----------------------------\n",
        "def get_augmentation_pipeline(intensity='medium'):\n",
        "    if intensity == 'light':\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        ])\n",
        "    elif intensity == 'medium':\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.05),\n",
        "            tf.keras.layers.RandomZoom(0.1),\n",
        "            tf.keras.layers.RandomBrightness(0.1),\n",
        "            tf.keras.layers.RandomContrast(0.1)\n",
        "        ])\n",
        "    elif intensity == 'strong':\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            tf.keras.layers.RandomRotation(0.15),\n",
        "            tf.keras.layers.RandomZoom(0.2),\n",
        "            tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
        "            tf.keras.layers.RandomContrast(0.3),\n",
        "            tf.keras.layers.RandomBrightness(0.2)\n",
        "        ])\n",
        "    else:\n",
        "        raise ValueError(\"Choose from: light | medium | strong\")\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing Function\n",
        "# -----------------------------\n",
        "def preprocess_image(img_path,\n",
        "                     label,\n",
        "                     img_size=(224, 224),\n",
        "                     channels=3,\n",
        "                     apply_clahe=False,\n",
        "                     augment=False,\n",
        "                     augment_level=\"medium\",\n",
        "                     normalize=True,\n",
        "                     num_classes=1):\n",
        "\n",
        "    image = tf.io.read_file(img_path)\n",
        "\n",
        "    # Auto-detect encoding (e.g. PNG or JPEG)\n",
        "    file_format = tf.strings.split(img_path, \".\")[-1]\n",
        "    image = tf.cond(tf.equal(file_format, \"png\"),\n",
        "                    lambda: tf.image.decode_png(image, channels=channels),\n",
        "                    lambda: tf.image.decode_jpeg(image, channels=channels))\n",
        "\n",
        "    image = tf.image.resize(image, img_size)\n",
        "\n",
        "    if apply_clahe:\n",
        "        image = apply_clahe_tf(image)\n",
        "    else:\n",
        "        image = tf.cast(image, tf.float32) / 255.0 if normalize else tf.cast(image, tf.float32)\n",
        "\n",
        "    if augment:\n",
        "        aug = get_augmentation_pipeline(augment_level)\n",
        "        image = aug(image, training=True)\n",
        "\n",
        "    if num_classes > 1:\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# -----------------------------\n",
        "# TF Dataset Builder\n",
        "# -----------------------------\n",
        "def get_tf_dataset(img_paths,\n",
        "                   labels,\n",
        "                   batch_size=32,\n",
        "                   shuffle_buffer=1024,\n",
        "                   img_size=(224, 224),\n",
        "                   channels=3,\n",
        "                   training=True,\n",
        "                   apply_clahe=False,\n",
        "                   augment_level=\"medium\",\n",
        "                   normalize=True,\n",
        "                   num_classes=1):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: preprocess_image(\n",
        "            x, y,\n",
        "            img_size=img_size,\n",
        "            channels=channels,\n",
        "            apply_clahe=apply_clahe,\n",
        "            augment=training,\n",
        "            augment_level=augment_level,\n",
        "            normalize=normalize,\n",
        "            num_classes=num_classes\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    duration = time.time() - start\n",
        "    print(f\" TF Dataset prepared in {duration:.2f} seconds.\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "SYnnO-ZjUVd0"
      },
      "id": "SYnnO-ZjUVd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"combined_dataset.csv\")\n",
        "image_paths = df['image_path'].values\n",
        "labels = df['label'].values  # binary or int class\n",
        "\n",
        "train_ds = get_tf_dataset(\n",
        "    img_paths=image_paths,\n",
        "    labels=labels,\n",
        "    batch_size=32,\n",
        "    training=True,\n",
        "    apply_clahe=True,\n",
        "    augment_level=\"strong\",\n",
        "    num_classes=2\n",
        ")\n",
        "\n",
        "val_ds = get_tf_dataset(\n",
        "    img_paths=image_paths,\n",
        "    labels=labels,\n",
        "    training=False,\n",
        "    num_classes=2\n",
        ")"
      ],
      "metadata": {
        "id": "yrHgAzB8UdFk"
      },
      "id": "yrHgAzB8UdFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c9dfa81",
      "metadata": {
        "id": "9c9dfa81"
      },
      "source": [
        "## 6. Build Base Models (VGG16, ResNet50, InceptionV3, DenseNet121, DenseNet201, AlexNet (via custom implementation), ViT-B/16 (Vision Transformer, Custom CNN (my own architecture))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c899f1",
      "metadata": {
        "id": "69c899f1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import (\n",
        "    VGG16, ResNet50, InceptionV3, DenseNet121, DenseNet201\n",
        ")\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "def build_base_model(name, input_shape=(224, 224, 3)):\n",
        "    \"\"\"\n",
        "    Returns a base model architecture with include_top=False.\n",
        "    \"\"\"\n",
        "    if name == 'VGG16':\n",
        "        return VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif name == 'ResNet50':\n",
        "        return ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif name == 'InceptionV3':\n",
        "        return InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif name == 'DenseNet121':\n",
        "        return DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif name == 'DenseNet201':\n",
        "        return DenseNet201(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif name == 'AlexNet':\n",
        "        return build_alexnet(input_shape)\n",
        "    elif name == 'ViT-B-16':\n",
        "        return build_vit_model(input_shape)\n",
        "    elif name == 'CustomCNN':\n",
        "        return build_custom_cnn(input_shape)\n",
        "    else:\n",
        "        raise ValueError(f\"Model '{name}' is not supported.\")\n",
        "\n",
        "\n",
        "def build_model_wrapper(model_name, input_shape=(224, 224, 3), num_classes=2, fine_tune=False):\n",
        "    base = build_base_model(model_name, input_shape)\n",
        "    if hasattr(base, 'trainable') and not fine_tune:\n",
        "        base.trainable = False\n",
        "\n",
        "    if model_name in ['AlexNet', 'CustomCNN']:  # Already includes classification head\n",
        "        return base\n",
        "\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    if num_classes == 1:\n",
        "        out = Dense(1, activation='sigmoid')(x)\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        out = Dense(num_classes, activation='softmax')(x)\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "    model = Model(inputs=base.input, outputs=out, name=f\"{model_name}_Model\")\n",
        "    model.compile(optimizer=Adam(1e-4), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vit_model(input_shape=(224, 224, 3), num_classes=2):\n",
        "    vit_url = \"https://tfhub.dev/google/vit/base_patch16_224/1\"\n",
        "    vit_layer = hub.KerasLayer(vit_url, trainable=False)\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = tf.image.resize(inputs, [224, 224])\n",
        "    x = vit_layer(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs, out, name=\"ViT_B16\")\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "R4zdQiSWV_F7"
      },
      "id": "R4zdQiSWV_F7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_custom_cnn(input_shape=(224, 224, 3), num_classes=2):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "AORSksf7WBmc"
      },
      "id": "AORSksf7WBmc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_alexnet(input_shape=(224, 224, 3), num_classes=2):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
        "        MaxPooling2D(pool_size=3, strides=2),\n",
        "        Conv2D(256, kernel_size=5, padding='same', activation='relu'),\n",
        "        MaxPooling2D(pool_size=3, strides=2),\n",
        "        Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n",
        "        Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n",
        "        Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n",
        "        MaxPooling2D(pool_size=3, strides=2),\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "jePGYa8XWDq9"
      },
      "id": "jePGYa8XWDq9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "184faf2c",
      "metadata": {
        "id": "184faf2c"
      },
      "source": [
        "## 7. Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_and_evaluate(model, model_name, train_ds, val_ds, test_ds, epochs=10, output_dir=\"results\"):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    weights_path = os.path.join(output_dir, f\"{model_name}_best_weights.h5\")\n",
        "\n",
        "    # Callback to save best weights\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        weights_path, monitor='val_accuracy', save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True, monitor='val_loss'\n",
        "    )\n",
        "\n",
        "    print(f\" Training model: {model_name}\")\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
        "    )\n",
        "\n",
        "    # Load best weights\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(f\"\\n Evaluating {model_name} on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_ds)\n",
        "    print(f\" Test Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Predict\n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch in test_ds:\n",
        "        preds = model.predict(x_batch)\n",
        "        y_true.extend(y_batch.numpy())\n",
        "        y_pred.extend(np.argmax(preds, axis=1) if preds.shape[1] > 1 else (preds > 0.5).astype(\"int\").flatten())\n",
        "\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\" Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history, model_name, output_dir)\n",
        "\n",
        "\n",
        "def plot_training_history(history, model_name, save_dir):\n",
        "    \"\"\"\n",
        "    Plots accuracy and loss curves from model training history.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val')\n",
        "    plt.title(f'{model_name} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Val')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, f\"{model_name}_training_plot.png\"))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XP38h0q5ZTC0"
      },
      "id": "XP38h0q5ZTC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52359094",
      "metadata": {
        "id": "52359094"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_names = [\"VGG16\", \"ResNet50\", \"InceptionV3\", \"DenseNet121\", \"DenseNet201\", \"AlexNet\", \"ViT-B-16\", \"CustomCNN\"]\n",
        "\n",
        "for name in model_names:\n",
        "    print(f\"ðŸ”§ Building {name}...\")\n",
        "    model = build_model_wrapper(name, input_shape=(224, 224, 3), num_classes=2)\n",
        "    model.summary()\n",
        "    # model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#  Define model names\n",
        "model_names = [\n",
        "    \"VGG16\",\n",
        "    \"ResNet50\",\n",
        "    \"InceptionV3\",\n",
        "    \"DenseNet121\",\n",
        "    \"DenseNet201\",\n",
        "    \"AlexNet\",\n",
        "    \"ViT-B-16\",\n",
        "    \"CustomCNN\"\n",
        "]\n",
        "\n",
        "\n",
        "#  Loop through and train + evaluate each model\n",
        "for name in model_names:\n",
        "    print(f\"\\n Starting model: {name}\")\n",
        "    try:\n",
        "        model = build_model_wrapper(\n",
        "            model_name=name,\n",
        "            input_shape=(224, 224, 3),\n",
        "            num_classes=2,\n",
        "            fine_tune=False\n",
        "        )\n",
        "\n",
        "        train_and_evaluate(\n",
        "            model=model,\n",
        "            model_name=name,\n",
        "            train_ds=train_ds,\n",
        "            val_ds=val_ds,\n",
        "            test_ds=test_ds,\n",
        "            epochs=10,\n",
        "            output_dir=f\"results/{name.lower()}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to train {name}: {e}\")\n"
      ],
      "metadata": {
        "id": "f8hA4KLCWJdd"
      },
      "id": "f8hA4KLCWJdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1e444fdf",
      "metadata": {
        "id": "1e444fdf"
      },
      "source": [
        "## 8. Stacking Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_model(model_class, input_shape=(224, 224, 3), name=\"base\"):\n",
        "    base = model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable = False\n",
        "    inputs = base.input\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D(name=f\"{name}_gap\")(x)\n",
        "    return Model(inputs=inputs, outputs=x, name=name)"
      ],
      "metadata": {
        "id": "RaRcoX4oZn8j"
      },
      "id": "RaRcoX4oZn8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_celm_model(input_shape=(224, 224, 3), num_classes=2):\n",
        "    # Instantiate base feature extractors\n",
        "    vgg = get_base_model(VGG16, input_shape, name=\"vgg16\")\n",
        "    resnet = get_base_model(ResNet50, input_shape, name=\"resnet50\")\n",
        "    inception = get_base_model(InceptionV3, input_shape, name=\"inceptionv3\")\n",
        "\n",
        "    # Define 3 separate inputs (but in practice, they'll be the same image)\n",
        "    input_vgg = Input(shape=input_shape, name='input_vgg')\n",
        "    input_res = Input(shape=input_shape, name='input_resnet')\n",
        "    input_inc = Input(shape=input_shape, name='input_inception')\n",
        "\n",
        "    # Get feature outputs\n",
        "    feat_vgg = vgg(input_vgg)\n",
        "    feat_res = resnet(input_res)\n",
        "    feat_inc = inception(input_inc)\n",
        "\n",
        "    # Concatenate features\n",
        "    concatenated = Concatenate(name=\"concat_features\")([feat_vgg, feat_res, feat_inc])\n",
        "    x = Dense(256, activation='relu', name='fc1')(concatenated)\n",
        "    x = Dropout(0.4, name='dropout')(x)\n",
        "\n",
        "    if num_classes == 1:\n",
        "        output = Dense(1, activation='sigmoid', name='output')(x)\n",
        "        loss_fn = 'binary_crossentropy'\n",
        "    else:\n",
        "        output = Dense(num_classes, activation='softmax', name='output')(x)\n",
        "        loss_fn = 'sparse_categorical_crossentropy'\n",
        "\n",
        "    model = Model(inputs=[input_vgg, input_res, input_inc], outputs=output, name='CELM')\n",
        "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "YF5_Q6xrZqCj"
      },
      "id": "YF5_Q6xrZqCj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_multi_input_dataset(single_input_dataset):\n",
        "    \"\"\"\n",
        "    Converts a (image, label) dataset into a multi-input tuple for stacking ensemble.\n",
        "    Returns dataset in format: ((img, img, img), label)\n",
        "    \"\"\"\n",
        "    return single_input_dataset.map(lambda x, y: ((x, x, x), y), num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "EzuY5IOaZufs"
      },
      "id": "EzuY5IOaZufs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa977eb",
      "metadata": {
        "id": "eaa977eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Collect model predictions and fit meta-learner\n",
        "def train_meta_model(base_preds, y_true):\n",
        "    meta_model = LogisticRegression()\n",
        "    meta_model.fit(base_preds, y_true)\n",
        "    return meta_model\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Training and Evaluation"
      ],
      "metadata": {
        "id": "DWLJ4RFZZzQj"
      },
      "id": "DWLJ4RFZZzQj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c177af1",
      "metadata": {
        "id": "1c177af1"
      },
      "outputs": [],
      "source": [
        "train_ds_multi = get_multi_input_dataset(train_ds)\n",
        "val_ds_multi = get_multi_input_dataset(val_ds)\n",
        "test_ds_multi = get_multi_input_dataset(test_ds)\n",
        "\n",
        "celm_model = build_celm_model(input_shape=(224, 224, 3), num_classes=2)\n",
        "\n",
        "from train_and_evaluate import train_and_evaluate\n",
        "\n",
        "train_and_evaluate(\n",
        "    model=celm_model,\n",
        "    model_name=\"CELM\",\n",
        "    train_ds=train_ds_multi,\n",
        "    val_ds=val_ds_multi,\n",
        "    test_ds=test_ds_multi,\n",
        "    epochs=10,\n",
        "    output_dir=\"results/celm\"\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1914658",
      "metadata": {
        "id": "b1914658"
      },
      "source": [
        "## 10. Save Models and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91ccd5c",
      "metadata": {
        "id": "c91ccd5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Export helper function\n",
        "# -----------------------------\n",
        "def export_model(model, model_name=\"model\", export_dir=\"exports\"):\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "    # HDF5 format\n",
        "    h5_path = os.path.join(export_dir, f\"{model_name}.h5\")\n",
        "    model.save(h5_path)\n",
        "    print(f\" {model_name} saved to HDF5: {h5_path}\")\n",
        "\n",
        "    # SavedModel format\n",
        "    savedmodel_path = os.path.join(export_dir, f\"{model_name}_savedmodel\")\n",
        "    model.save(savedmodel_path, save_format='tf')\n",
        "    print(f\" {model_name} saved as SavedModel: {savedmodel_path}\")\n",
        "\n",
        "    # TFLite format\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(savedmodel_path)\n",
        "    tflite_model = converter.convert()\n",
        "    tflite_path = os.path.join(export_dir, f\"{model_name}.tflite\")\n",
        "    with open(tflite_path, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    print(f\" {model_name} exported to TensorFlow Lite: {tflite_path}\\n\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Define model names\n",
        "# -----------------------------\n",
        "model_names = [\n",
        "    \"VGG16\",\n",
        "    \"ResNet50\",\n",
        "    \"InceptionV3\",\n",
        "    \"DenseNet121\",\n",
        "    \"DenseNet201\",\n",
        "    \"AlexNet\",\n",
        "    \"ViT-B-16\",\n",
        "    \"CustomCNN\"\n",
        "]\n",
        "\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # -----------------------------\n",
        "#  Loop over each model\n",
        "# -----------------------------\n",
        "for name in model_names:\n",
        "    print(f\"\\n Exporting model: {name}\")\n",
        "    try:\n",
        "        model = build_model_wrapper(name, input_shape=input_shape, num_classes=num_classes)\n",
        "        weights_path = f\"results/{name.lower()}/{name}_best_weights.h5\"\n",
        "        model.load_weights(weights_path)\n",
        "        export_model(model, model_name=name, export_dir=f\"exports/{name.lower()}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Could not export {name}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "#  Handle CELM (Stacking Ensemble)\n",
        "# -----------------------------\n",
        "print(\"\\n Exporting CELM ensemble model...\")\n",
        "try:\n",
        "    celm_model = build_celm_model(input_shape=input_shape, num_classes=num_classes)\n",
        "    celm_model.load_weights(\"results/celm/CELM_best_weights.h5\")\n",
        "    export_model(celm_model, model_name=\"CELM\", export_dir=\"exports/celm\")\n",
        "except Exception as e:\n",
        "    print(f\" Could not export CELM: {e}\")"
      ],
      "metadata": {
        "id": "oLLaKQpcaMe0"
      },
      "id": "oLLaKQpcaMe0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}